# Growth Analysis Performance Optimization Plan

**Date:** 2025-11-21 (Updated: 2025-11-25)  
**Status:** ‚úÖ **COMPLETED**  
**Context:** Post v6.1 test data generation success (144/144 batches generated)

---

## üéØ Problem Statement

**Growth analysis recomputation times out after 300 seconds per batch** (expected: 30-60 seconds).

**Impact:**
- Batch generation: ‚úÖ No impact (100% stable, 144/144 success)
- Operational data: ‚úÖ Complete and usable (18.6M events)
- Growth Analysis UI: ‚ùå Orange "Actual Daily State" line empty

**Symptoms:**
- Subprocess workers timeout during `recompute_batch_assignments()` call
- All 144 batches timeout (not a data-specific issue)
- CPU load shows tight loops (14 workers pegged at 100%)
- Suggests query inefficiency or N+1 problem

---

## ‚úÖ SOLUTION IMPLEMENTED (2025-11-25)

### Summary

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Time per batch | 300s+ (timeout) | **3.2s** | **150x faster** |
| Query count | ~40,000+ | ~50 | **800x fewer** |
| Total time (145 batches) | ‚àû (timeout) | **7.8 minutes** | ‚úÖ Working |
| Success rate | 0% | **100%** | ‚úÖ Fixed |

### What Was Done

#### 1. Database Indexes (Migration)

Created two migration files to add composite indexes for frequently queried columns:

**`apps/batch/migrations/0038_add_growth_analysis_performance_indexes.py`:**
- `MortalityEvent`: `(assignment, event_date)`, `(batch, event_date)`
- `TransferAction`: `(source_assignment, actual_execution_date, status)`, `(dest_assignment, ...)`
- `BatchContainerAssignment`: `(batch, assignment_date, departure_date)`
- `GrowthSample`: `(assignment, sample_date)`, `(batch, sample_date)`

**`apps/inventory/migrations/0015_add_feeding_performance_indexes.py`:**
- `FeedingEvent`: `(container, feeding_date)`, `(batch, feeding_date)`

**Production Benefit:** These indexes improve performance for ALL batch queries, not just test data generation. Daily Celery-triggered Growth Analysis updates will also benefit.

#### 2. Bulk Query Engine (`growth_assimilation_optimized.py`)

Created new optimized service that replaces the N+1 query pattern:

**Old Pattern (N+1 Problem):**
```python
for day in range(800):
    mortality = MortalityEvent.objects.filter(assignment=a, event_date=day)  # Query 1
    feeding = FeedingEvent.objects.filter(container=c, feeding_date=day)      # Query 2
    transfers = TransferAction.objects.filter(...)                           # Query 3
    # ... 40,000+ queries per batch
```

**New Pattern (Bulk Queries):**
```python
# 5 queries total for entire date range
mortality = MortalityEvent.objects.filter(assignment=a, event_date__range=(start, end))
feeding = FeedingEvent.objects.filter(container=c, feeding_date__range=(start, end))
# ... process in memory, then bulk_create/bulk_update
```

**Key Features:**
- `_get_all_events_in_range()`: Fetches all event types in ~5 queries
- In-memory daily state computation
- `bulk_create()` / `bulk_update()` for ActualDailyAssignmentState
- Proper initial weight handling for transfers (fixed stage transition spikes)

#### 3. Stage Transition Spike Fix

**Bug:** Event engine sets incorrect `avg_weight_g` on destination assignments during transfers. Adult stage assignments were initialized with ~3300g (stage constraint minimum) instead of actual transferred weight (~500g from Post-Smolt).

**Fix:** Growth assimilation now checks transfer actions FIRST, uses source assignment's last computed weight:

```python
def _get_initial_weight(self):
    # Priority 1: Check transfers FIRST (fixes spike bug)
    transfer_in = TransferAction.objects.filter(dest_assignment=self.assignment).first()
    if transfer_in:
        # Use source assignment's weight, not stage constraint
        last_state = ActualDailyAssignmentState.objects.filter(
            assignment=transfer_in.source_assignment
        ).order_by('-date').first()
        if last_state:
            return float(last_state.avg_weight_g)
    
    # Priority 2: Assignment's avg_weight_g (only for non-transfers)
    if self.assignment.avg_weight_g:
        return float(self.assignment.avg_weight_g)
    # ...
```

**Result:** Smooth weight progression across stage transitions (Post-Smolt 520g ‚Üí Adult 530g instead of spike to 3300g).

#### 4. Standalone Runner Script

Created `scripts/data_generation/run_growth_analysis_optimized.py`:

```bash
# Run after batch generation completes
python scripts/data_generation/run_growth_analysis_optimized.py --workers 4
```

**Features:**
- Parallel processing with ProcessPoolExecutor
- Per-batch logging
- Progress reporting
- Error handling

---

## üìÅ Files Changed

| File | Change |
|------|--------|
| `apps/batch/services/growth_assimilation_optimized.py` | **NEW** - Bulk query engine |
| `apps/batch/services/growth_assimilation.py` | Updated initial weight logic |
| `apps/batch/migrations/0038_add_growth_analysis_performance_indexes.py` | **NEW** - DB indexes |
| `apps/inventory/migrations/0015_add_feeding_performance_indexes.py` | **NEW** - Feeding indexes |
| `scripts/data_generation/run_growth_analysis_optimized.py` | **NEW** - Standalone runner |
| `scripts/data_generation/04_batch_orchestrator_parallel.py` | Uses optimized engine |

---

## üìã Success Criteria - ALL MET ‚úÖ

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| Time per batch | <60s | **3.2s** | ‚úÖ |
| Query count | <1,000 | ~50 | ‚úÖ |
| CPU pattern | Bursty I/O | ‚úÖ Confirmed | ‚úÖ |
| Success rate | 100% | **145/145** | ‚úÖ |
| Total time (all batches) | <2 hours | **7.8 minutes** | ‚úÖ |
| Stage transition spikes | None | Fixed | ‚úÖ |

---

## üöÄ Usage (For Future Agents)

### Test Data Generation Workflow

```bash
cd /Users/aquarian247/Projects/AquaMind

# 1. Apply migrations (includes Growth Analysis indexes)
python manage.py migrate

# 2. Generate batch data (45-60 minutes)
SKIP_CELERY_SIGNALS=1 python scripts/data_generation/execute_batch_schedule.py \
  config/schedule_production.yaml --workers 14 --use-partitions

# 3. Run Growth Analysis recomputation (8-10 minutes) - REQUIRED!
python scripts/data_generation/run_growth_analysis_optimized.py --workers 4
```

**‚ö†Ô∏è CRITICAL:** Step 3 is required for Growth Analysis charts to work. Without it, the "Actual Daily State" line will be empty.

### Verify Success

```bash
DJANGO_SETTINGS_MODULE=aquamind.settings python -c "
import django; django.setup()
from apps.batch.models import ActualDailyAssignmentState
print(f'ActualDailyAssignmentState records: {ActualDailyAssignmentState.objects.count():,}')
"
# Expected: ~940,000 records for 145 batches
```

---

## üîç Original Root Cause Analysis (For Reference)

The N+1 query hypothesis was **confirmed**:

- Each 800-day batch was executing ~40,000+ database queries
- Each day queried: feeding, mortality, transfers, growth samples, environmental
- Result: 800 days √ó 10 containers √ó 5 event types = 40,000+ queries
- At 0.01s per query = 400+ seconds (matches observed timeout!)

The fix reduces this to ~50 bulk queries that fetch all data for the entire date range, then process in memory.

---

## ‚úÖ No Further Action Required

This optimization plan is **COMPLETE**. The Growth Analysis system is now:
- ‚úÖ Fast (150x speedup)
- ‚úÖ Stable (100% success rate)
- ‚úÖ Accurate (no stage transition spikes)
- ‚úÖ Production-ready (indexes benefit daily operations too)
