# App Batch Code Review Findings

## Summary
- Batch analytics endpoints reference legacy `population_count` and `biomass_kg` fields that were removed from `Batch`, so any call into `performance_metrics` or its helpers now raises `AttributeError`.
- Filter definitions, especially `BatchFilter`, still expose range filters that target the removed columns, meaning consumers receive `FieldError` as soon as those filters are used.
- `GrowthSampleSerializer` invokes its validation helpers with incorrect arguments and even returns raw error dicts, so create/update requests consistently fail before reaching model logic.
- Assignment and transfer paths bypass core invariants: validation helpers never see the computed biomass value, transfers can drive counts negative without correcting biomass, and no automated coverage guards these regressions.

## Suggested Mitigations

### Restore Analytics Stability
- Update `BatchAnalyticsMixin` (and tests) to pull `calculated_population_count`, `calculated_biomass_kg`, and the `batch_assignments` related name instead of accessing removed model fields.
- Introduce temporary guards (e.g., `getattr(batch, "calculated_population_count", 0)`) so endpoints degrade gracefully while refactoring is underway.
- Add regression tests that exercise `performance_metrics`, `growth_analysis`, and `compare` to ensure analytics calls succeed across batches with and without samples.

### Align Filtering With Calculated Fields
- Replace `biomass_*` and `population_*` filters with annotated queryset fields or custom filter methods that reuse the calculated properties.
- Add an integration test that hits `/api/v1/batch/batches` with the documented filter parameters to prevent future schema drift.

### Repair Growth Sample Validation
- Fix the call signatures for `validate_individual_measurements` and `validate_min_max_weight`, ensuring they align with the helper definitions.
- Normalize validation error handling so serializers raise `ValidationError` instead of returning raw dicts; encode this expectation in unit tests covering both measurement-based and manual payloads.
- Re-run the service workflow to confirm assignments receive `last_weighing_date` updates after these fixes.

### Harden Assignment & Transfer Workflows
- Feed the computed biomass value into `validate_container_capacity` so capacity checks actually run; consider caching the calculated biomass on the serializer instance to avoid recomputation.
- Guard transfer updates by clamping population counts at zero and recomputing biomass from the surviving fish; add tests to prove mortality and over-transfer scenarios behave correctly.
- Expand coverage around mixed-batch compositions to verify population and biomass accounting stays consistent when splits/merges occur.

### Testing & Monitoring Follow-Up
- Backfill missing API tests for the analytics actions and add serializer-level tests for growth samples that include individual measurements.
- Consider lightweight logging or metrics around transfer saves so unexpected negative populations are surfaced quickly.

_Feel free to iterate on the proposed test coverage or introduce alternative validation strategies; the key is to reconnect the analytics layer to the new data model and re-establish the guardrails around growth sampling and transfers._
